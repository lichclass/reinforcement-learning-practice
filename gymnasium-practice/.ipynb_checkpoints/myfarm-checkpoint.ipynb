{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1644f6b0-dee8-4ffb-8b3b-07da7796159d",
   "metadata": {},
   "source": [
    "# **Welcome to my Farm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24f1e50-1497-4267-81fe-30297298e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class FarmGridWorldEnv(gym.Env):\n",
    "    def __init__(self, size: int = 5, harvest_goal: int = 10):\n",
    "        \"\"\"\n",
    "        Observations (Discrete but stored in a Dict):\n",
    "          - agent_loc, grid_rep, crop_timer, soil_moisture, dry_counter, harvest_count, fertility_rep\n",
    "        Actions:\n",
    "          - 0..8 => Movement (including diagonals, stop)\n",
    "          - 9 => Plough\n",
    "          - 10 => Plant\n",
    "          - 11 => Water\n",
    "          - 12 => Harvest\n",
    "        \"\"\"\n",
    "\n",
    "        # Grid Size (n x n)\n",
    "        self.size = size\n",
    "        self.goal = harvest_goal\n",
    "\n",
    "        # Agent location\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Grids\n",
    "        self._grid = np.zeros((size, size), dtype=np.int32)\n",
    "        self._crop_timer_grid = np.zeros((size, size), dtype=np.int32)\n",
    "        self._soil_moisture_grid = np.zeros((size, size), dtype=np.int32)\n",
    "        self._dry_counter_grid = np.zeros((size, size), dtype=np.int32)\n",
    "        self._fertility_grid = np.zeros((size, size), dtype=np.int32)\n",
    "\n",
    "        # For tracking how many crops have been harvested\n",
    "        self._harvested = 0\n",
    "\n",
    "        # Track visits (for heatmap & \"touching unused parts\" reward)\n",
    "        self._usage_grid = np.zeros((size, size), dtype=np.int32)\n",
    "\n",
    "        # Configurations\n",
    "        self._max_crop_timer = 30\n",
    "        self._max_soil_moisture = 15\n",
    "        self._max_dry_counter = 10\n",
    "        self._max_fertility = 30\n",
    "\n",
    "        # Spaces\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"agent_loc\":     gym.spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int32),\n",
    "            \"grid_rep\":      gym.spaces.Box(low=0, high=3, shape=(size,size), dtype=np.int32),\n",
    "            \"crop_timer_rep\":gym.spaces.Box(low=0, high=self._max_crop_timer, shape=(size,size), dtype=np.int32),\n",
    "            \"soil_moisture_rep\": gym.spaces.Box(low=0, high=self._max_soil_moisture, shape=(size,size), dtype=np.int32),\n",
    "            \"dry_counter_rep\":   gym.spaces.Box(low=0, high=self._max_dry_counter, shape=(size,size), dtype=np.int32),\n",
    "            \"harvest_count\":     gym.spaces.Box(low=0, high=harvest_goal, shape=(), dtype=np.int32),\n",
    "            \"fertility_rep\":     gym.spaces.Box(low=0, high=self._max_fertility, shape=(size, size), dtype=np.int32),\n",
    "        })\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(13)\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([-1,  0]), # up\n",
    "            1: np.array([ 1,  0]), # down\n",
    "            2: np.array([ 0, -1]), # left\n",
    "            3: np.array([ 0,  1]), # right\n",
    "            4: np.array([ 0,  0]), # stop/no movement\n",
    "            5: np.array([-1,  1]), # top-right\n",
    "            6: np.array([-1, -1]), # top-left\n",
    "            7: np.array([ 1,  1]), # bottom-right\n",
    "            8: np.array([ 1, -1]), # bottom-left\n",
    "        }\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent_loc\":       self._agent_location.astype(np.int32),\n",
    "            \"grid_rep\":        self._grid.astype(np.int32),\n",
    "            \"crop_timer_rep\":  self._crop_timer_grid.astype(np.int32),\n",
    "            \"soil_moisture_rep\": self._soil_moisture_grid.astype(np.int32),\n",
    "            \"dry_counter_rep\": self._dry_counter_grid.astype(np.int32),\n",
    "            \"harvest_count\":   self._harvested,\n",
    "            \"fertility_rep\":   self._fertility_grid.astype(np.int32),\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"harvest_goal\": self.goal,\n",
    "            \"usage_grid\":   self._usage_grid.copy(),\n",
    "        }\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Random agent location\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=np.int32)\n",
    "\n",
    "        self._harvested = 0\n",
    "\n",
    "        # Reset grids\n",
    "        self._grid.fill(0)\n",
    "        self._crop_timer_grid.fill(0)\n",
    "        self._dry_counter_grid.fill(0)\n",
    "\n",
    "        # Random initial moisture\n",
    "        self._soil_moisture_grid = np.random.randint(\n",
    "            0, self._max_soil_moisture + 1, size=(self.size, self.size), dtype=np.int32\n",
    "        )\n",
    "        # Random fertility from 10..30\n",
    "        self._fertility_grid = np.random.randint(\n",
    "            10, self._max_fertility + 1, size=(self.size, self.size), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def _update_crop_growth(self):\n",
    "        growing_crops = (self._grid == 2)\n",
    "        moist_soils = (self._soil_moisture_grid > 0)\n",
    "\n",
    "        can_grow = growing_crops & moist_soils\n",
    "        self._crop_timer_grid[can_grow] -= 1\n",
    "\n",
    "        fully_grown = (self._crop_timer_grid <= 0) & can_grow\n",
    "        self._grid[fully_grown] = 3\n",
    "\n",
    "    def _decay_soil_moisture(self):\n",
    "        has_moisture = (self._soil_moisture_grid > 0)\n",
    "        self._soil_moisture_grid[has_moisture] -= 1\n",
    "\n",
    "    def _handle_crop_death(self, penalty: int = -5):\n",
    "        \"\"\"Kill crops if dryness is too long, apply penalty per dead crop.\"\"\"\n",
    "        planted = (self._grid == 2)\n",
    "        dry = (self._soil_moisture_grid == 0)\n",
    "        dry_crops = planted & dry\n",
    "\n",
    "        self._dry_counter_grid[dry_crops] += 1\n",
    "        rehydrated = planted & (self._soil_moisture_grid > 0)\n",
    "        self._dry_counter_grid[rehydrated] = 0\n",
    "\n",
    "        dead_crops = (self._dry_counter_grid >= 10) & planted\n",
    "        num_dead = np.count_nonzero(dead_crops)\n",
    "        if num_dead > 0:\n",
    "            self._grid[dead_crops] = 0\n",
    "            self._crop_timer_grid[dead_crops] = 0\n",
    "            self._soil_moisture_grid[dead_crops] = 0\n",
    "            self._dry_counter_grid[dead_crops] = 0\n",
    "\n",
    "        return penalty * num_dead\n",
    "\n",
    "    def _recover_fertility(self):\n",
    "        # Soils that are empty(0) or ploughed(1) can recover up to max\n",
    "        empty_or_ploughed = (self._grid == 0) | (self._grid == 1)\n",
    "        needs_recovery = empty_or_ploughed & (self._fertility_grid < self._max_fertility)\n",
    "\n",
    "        self._fertility_grid[needs_recovery] += 1\n",
    "        above_max = self._fertility_grid > self._max_fertility\n",
    "        self._fertility_grid[above_max] = self._max_fertility\n",
    "\n",
    "    def _fertility_harvest_bonus(self, x, y, default_reward):\n",
    "        \"\"\"Scale harvest reward by fertility, degrade a bit.\"\"\"\n",
    "        fertility_factor = self._fertility_grid[x, y] / float(self._max_fertility)\n",
    "        actual_reward = default_reward * fertility_factor\n",
    "        self._fertility_grid[x, y] = max(0, self._fertility_grid[x, y] - 3)\n",
    "        return actual_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -0.001   # baseline cost per step\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "\n",
    "        x, y = self._agent_location\n",
    "\n",
    "        # (1) REWARD FOR TOUCHING UNUSED PARTS:\n",
    "        #    If usage_grid[x,y]==0 BEFORE we move here, then +1\n",
    "        if self._usage_grid[x, y] == 0:\n",
    "            reward += 1.0  # exploring a new tile\n",
    "        self._usage_grid[x, y] += 1\n",
    "\n",
    "        # Movement\n",
    "        if action in self._action_to_direction:\n",
    "            direction = self._action_to_direction[action]\n",
    "            old_x, old_y = x, y\n",
    "            # Clip so we don't go out of bounds\n",
    "            self._agent_location = np.clip(self._agent_location + direction, 0, self.size-1)\n",
    "            x, y = self._agent_location\n",
    "\n",
    "        # Define base rewards\n",
    "        action_rewards = {\n",
    "            \"plough\":      5,\n",
    "            \"plant\":       10,\n",
    "            \"water\":       8,\n",
    "            \"re_water\":    5,\n",
    "            \"harvest\":     200,\n",
    "        }\n",
    "        penalties = {\n",
    "            \"plough_inv\":   -2,\n",
    "            \"plant_inv\":    -2,\n",
    "            \"water_inv\":    -2,\n",
    "            \"harvest_inv\":  -10,\n",
    "            \"dead_crop\":    -5,\n",
    "        }\n",
    "\n",
    "        # Farming actions\n",
    "        if action == 9:   # Plough\n",
    "            if self._grid[x, y] == 0:\n",
    "                self._grid[x, y] = 1\n",
    "                reward += action_rewards[\"plough\"]\n",
    "            else:\n",
    "                reward += penalties[\"plough_inv\"]\n",
    "\n",
    "        elif action == 10: # Plant\n",
    "            if self._grid[x, y] == 1:\n",
    "                self._grid[x, y] = 2\n",
    "                reward += action_rewards[\"plant\"]\n",
    "            else:\n",
    "                reward += penalties[\"plant_inv\"]\n",
    "\n",
    "        elif action == 11: # Water\n",
    "            if self._grid[x, y] == 2:\n",
    "                if self._soil_moisture_grid[x, y] > 0:\n",
    "                    reward += action_rewards[\"re_water\"]\n",
    "                else:\n",
    "                    reward += action_rewards[\"water\"]\n",
    "                    self._crop_timer_grid[x, y] = self._max_crop_timer\n",
    "                self._soil_moisture_grid[x, y] = self._max_soil_moisture\n",
    "            else:\n",
    "                reward += penalties[\"water_inv\"]\n",
    "\n",
    "        elif action == 12: # Harvest\n",
    "            if self._grid[x, y] == 3:\n",
    "                harvest_reward = self._fertility_harvest_bonus(x, y, action_rewards[\"harvest\"])\n",
    "                reward += harvest_reward\n",
    "\n",
    "                # reset tile\n",
    "                self._grid[x, y] = 0\n",
    "                self._crop_timer_grid[x, y] = 0\n",
    "                self._soil_moisture_grid[x, y] = 0\n",
    "\n",
    "                self._harvested += 1\n",
    "                # (3) REWARD for achieving harvest goal quickly:\n",
    "                if self._harvested >= self.goal:\n",
    "                    # bonus that scales inversely with steps taken\n",
    "                    # e.g., bigger if the agent finishes early\n",
    "                    # or just a flat big bonus\n",
    "                    quick_bonus = 500.0  # or (1000 - step_count), your call\n",
    "                    reward += quick_bonus\n",
    "                    terminated = True\n",
    "            else:\n",
    "                reward += penalties[\"harvest_inv\"]\n",
    "\n",
    "        # (2) REWARD FOR NURTURING A CROP:\n",
    "        # e.g., each step, if you have any planted tile that is moist, small +0.05\n",
    "        # so we do that after the environment updates:\n",
    "        # We'll do it with how many planted crops are still alive & moist\n",
    "        # We'll compute this after the environment updates.\n",
    "\n",
    "        # environment updates\n",
    "        self._update_crop_growth()\n",
    "        self._decay_soil_moisture()\n",
    "        dead_crop_penalty = self._handle_crop_death(penalty=penalties[\"dead_crop\"])\n",
    "        reward += dead_crop_penalty\n",
    "        self._recover_fertility()\n",
    "\n",
    "        # Now do the \"nurture reward\"\n",
    "        # Count how many crops are still in state=2 or 3 and have moisture>0\n",
    "        planted_or_grown = (self._grid == 2) | (self._grid == 3)\n",
    "        moist_tiles = (self._soil_moisture_grid > 0)\n",
    "        # Crop that is alive+moist\n",
    "        nurturing_mask = planted_or_grown & moist_tiles\n",
    "        num_nurtured = np.count_nonzero(nurturing_mask)\n",
    "        # small reward for each tile that is successfully nurtured\n",
    "        reward += 0.05 * num_nurtured\n",
    "\n",
    "        return self._get_obs(), float(reward), terminated, truncated, self._get_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe89d14-2cdf-46b4-98dd-21b169fd03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register environment\n",
    "\n",
    "gym.register(\n",
    "    id=\"gymnasium_env/FarmGridWorld-v0\",\n",
    "    entry_point=FarmGridWorldEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8ce1c0-0e1b-464a-bdf4-fe864aa1536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dash:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "        \n",
    "\n",
    "    def get_action(self, obs: dict) -> int:\n",
    "        \n",
    "        state_key = self._obs_to_key(obs)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[state_key]))\n",
    "            \n",
    "\n",
    "    def _obs_to_key(self, obs: dict) -> tuple:\n",
    "        \n",
    "        x, y = obs[\"agent_loc\"]\n",
    "        return (\n",
    "            int(x),\n",
    "            int(y),\n",
    "            int(obs[\"grid_rep\"][x, y]),\n",
    "            int(obs[\"crop_timer_rep\"][x, y]),\n",
    "            int(obs[\"soil_moisture_rep\"][x, y]),\n",
    "            int(obs[\"dry_counter_rep\"][x, y]),\n",
    "            int(obs[\"harvest_count\"]),\n",
    "            int(obs[\"fertility_rep\"][x, y]),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def update(\n",
    "            self,\n",
    "            obs: dict,\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            terminated: bool,\n",
    "            next_obs: dict,\n",
    "    ):\n",
    "        \n",
    "        state_key = self._obs_to_key(obs)\n",
    "        next_state_key = self._obs_to_key(next_obs)\n",
    "        \n",
    "        future_q = (not terminated) * np.max(self.q_values[next_state_key])\n",
    "    \n",
    "        td = reward + self.discount_factor * future_q - self.q_values[state_key][action]\n",
    "\n",
    "        self.q_values[state_key][action] += self.lr * td\n",
    "        self.training_error.append(td)\n",
    "        \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "    \n",
    "    def train(self, n_episodes=10_000):\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        total_harvest = 0\n",
    "        early_termination_count = 0\n",
    "        time_limit_count = 0\n",
    "\n",
    "        for episode in tqdm(range(n_episodes)):\n",
    "            obs, info = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            length = 0\n",
    "            episode_harvest = 0\n",
    "        \n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "                self.update(obs, action, reward, terminated, next_obs)\n",
    "        \n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "        \n",
    "                total_reward += reward\n",
    "                length += 1\n",
    "                episode_harvest = obs[\"harvest_count\"]\n",
    "                \n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_lengths.append(length)\n",
    "            total_harvest += episode_harvest\n",
    "            agent.decay_epsilon()\n",
    "        \n",
    "            if terminated:\n",
    "                early_termination_count += 1\n",
    "            elif truncated:\n",
    "                time_limit_count += 1\n",
    "        \n",
    "        print(\"Episodes ended by harvest goal (TRAIN):\", early_termination_count)\n",
    "        print(\"Episodes ended by time limit (TRAIN):\", time_limit_count)\n",
    "\n",
    "        return {\n",
    "            \"episode_rewards\": episode_rewards,\n",
    "            \"episode_lenghts\": episode_lengths,\n",
    "            \"total_harvest\": total_harvest,\n",
    "            \"early_termination_count\": early_termination_count,\n",
    "            \"time_limit_count\": time_limit_count,\n",
    "        }\n",
    "\n",
    "    def evaluate(self, test_n_episodes=10_000):\n",
    "\n",
    "        old_epsilon = self.epsilon\n",
    "        old_final_epsilon = self.final_epsilon\n",
    "        old_epsilon_decay = self.epsilon_decay\n",
    "        \n",
    "        self.epsilon = 0.0\n",
    "        self.final_epsilon = 0.0\n",
    "        self.epsilon_decay = 1.0\n",
    "        \n",
    "        test_episode_rewards = []\n",
    "        test_episode_lengths = []\n",
    "        test_total_harvest = 0\n",
    "        test_early_termination_count = 0\n",
    "        test_time_limit_count = 0\n",
    "        \n",
    "        for episode in tqdm(range(test_n_episodes)):\n",
    "            obs, info = env.reset()\n",
    "            done = False\n",
    "            test_total_reward = 0\n",
    "            test_length = 0\n",
    "            test_episode_harvest = 0\n",
    "        \n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "        \n",
    "                test_total_reward += reward\n",
    "                test_length += 1\n",
    "                test_episode_harvest = obs[\"harvest_count\"]\n",
    "                \n",
    "            test_episode_rewards.append(test_total_reward)\n",
    "            test_episode_lengths.append(test_length)\n",
    "            test_total_harvest += test_episode_harvest\n",
    "\n",
    "            if episode < 10:\n",
    "                print(f\"Reward for Episode {episode}: {test_total_reward}\")\n",
    "        \n",
    "            if terminated:\n",
    "                test_early_termination_count += 1\n",
    "            elif truncated:\n",
    "                test_time_limit_count += 1\n",
    "\n",
    "        self.epsilon = old_epsilon\n",
    "        self.final_epsilon = old_final_epsilon\n",
    "        self.epsilon_decay = old_epsilon_decay\n",
    "        \n",
    "        print(\"Episodes ended by harvest goal (TEST):\", test_early_termination_count)\n",
    "        print(\"Episodes ended by time limit (TEST):\", test_time_limit_count)\n",
    "\n",
    "        return {\n",
    "            \"test_episode_rewards\": test_episode_rewards,\n",
    "            \"test_episode_lengths\": test_episode_lengths,\n",
    "            \"test_total_harvest\": test_total_harvest,\n",
    "            \"test_early_termination_count\": test_early_termination_count,\n",
    "            \"test_time_limit_count\": test_time_limit_count,\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e022d9-2b44-4246-a106-3a0d5783ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "max_episode_steps = 1000\n",
    "size = 5\n",
    "harvest_goal = 10\n",
    "\n",
    "env = gym.wrappers.TimeLimit(\n",
    "    gym.make(\"gymnasium_env/FarmGridWorld-v0\", size=size, harvest_goal=harvest_goal),\n",
    "    max_episode_steps=max_episode_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900d2b44-58df-401b-99f8-110c0f21cc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/gymnasium/spaces/box.py:423: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_episodes = 500_000\n",
    "start_epsilon = 1.0\n",
    "final_epsilon = 0.1\n",
    "decay_episodes = int(n_episodes * 0.90)\n",
    "epsilon_decay = (final_epsilon / start_epsilon) ** (1.0 / decay_episodes)\n",
    "\n",
    "learning_rate = 0.01    \n",
    "discount_factor = 0.99\n",
    "\n",
    "# Agent Initialization\n",
    "state, info = env.reset()\n",
    "\n",
    "agent = Dash(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    discount_factor=discount_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa050f22-6536-4350-88fb-1a210af8342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███                               | 44153/500000 [29:46<5:07:28, 24.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m episode_lengths \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_lenghts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 90\u001b[0m, in \u001b[0;36mDash.train\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     89\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(obs)\n\u001b[0;32m---> 90\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n\u001b[1;32m     94\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.10/site-packages/gymnasium/wrappers/common.py:281\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment that on the first call will run the `passive_env_step_check`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "info = agent.train(n_episodes)\n",
    "\n",
    "episode_rewards = info[\"episode_rewards\"]\n",
    "episode_lengths = info[\"episode_lenghts\"]\n",
    "total_harvest = info[\"total_harvest\"]\n",
    "early_termination_count = info[\"early_termination_count\"]\n",
    "time_limit_count = info[\"time_limit_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4a610-914a-4d04-a583-5600ef5691a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Harvest (All Episodes): {total_harvest}\")\n",
    "\n",
    "def get_moving_avgs(arr, window, convolution_mode=\"valid\"):\n",
    "    return np.convolve(\n",
    "        np.array(arr).flatten(),\n",
    "        np.ones(window),\n",
    "        mode=convolution_mode\n",
    "    ) / window\n",
    "\n",
    "# Plotting\n",
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Episode rewards\n",
    "axs[0].set_title(\"Episode Rewards (Moving Avg)\")\n",
    "rewards_avg = get_moving_avgs(episode_rewards, rolling_length)\n",
    "axs[0].plot(range(len(rewards_avg)), rewards_avg)\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "# Episode lengths\n",
    "axs[1].set_title(\"Episode Lengths (Moving Avg)\")\n",
    "lengths_avg = get_moving_avgs(episode_lengths, rolling_length)\n",
    "axs[1].plot(range(len(lengths_avg)), lengths_avg)\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Steps\")\n",
    "\n",
    "# Training error (TD Error)\n",
    "axs[2].set_title(\"Training Error (Moving Avg)\")\n",
    "error_avg = get_moving_avgs(agent.training_error, rolling_length, \"same\")\n",
    "axs[2].plot(range(len(error_avg)), error_avg)\n",
    "axs[2].set_xlabel(\"Step\")\n",
    "axs[2].set_ylabel(\"TD Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067b049-acb9-4bd4-a828-4f82b24fdd6b",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd0609-4257-4eeb-b719-695696eba178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n_episodes = 5000\n",
    "\n",
    "eval_info = agent.evaluate(test_n_episodes)\n",
    "\n",
    "test_episode_rewards = eval_info[\"test_episode_rewards\"]\n",
    "test_episode_lengths = eval_info[\"test_episode_lengths\"]\n",
    "test_total_harvest = eval_info[\"test_total_harvest\"]\n",
    "test_early_termination_count = eval_info[\"test_early_termination_count\"]\n",
    "test_time_limit_count = eval_info[\"test_time_limit_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a22e9-b89d-4979-959f-8dcdf92d517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "final_obs, final_info = env.reset()\n",
    "usage_grid = final_info[\"usage_grid\"]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(usage_grid, annot=True, fmt=\"d\", cmap=\"YlGnBu\", cbar_kws={'label': 'Visit Count'})\n",
    "plt.title(\"Grid Usage Heatmap (Agent Visit Frequency)\")\n",
    "plt.xlabel(\"Column\")\n",
    "plt.ylabel(\"Row\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33988c8-3b08-42ec-ae04-b04e6490377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Harvest (All Episodes): {test_total_harvest}\")\n",
    "\n",
    "def get_moving_avgs(arr, window, convolution_mode=\"valid\"):\n",
    "    return np.convolve(\n",
    "        np.array(arr).flatten(),\n",
    "        np.ones(window),\n",
    "        mode=convolution_mode\n",
    "    ) / window\n",
    "\n",
    "# Plotting\n",
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Compute average reward per step for each episode\n",
    "avg_reward_per_step = [\n",
    "    reward / steps if steps > 0 else 0\n",
    "    for reward, steps in zip(test_episode_rewards, test_episode_lengths)\n",
    "]\n",
    "\n",
    "# Episode rewards\n",
    "axs[0].set_title(\"Episode Rewards (Moving Avg)\")\n",
    "rewards_avg = get_moving_avgs(test_episode_rewards, rolling_length)\n",
    "axs[0].plot(range(len(rewards_avg)), rewards_avg)\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "# Episode lengths\n",
    "axs[1].set_title(\"Episode Lengths (Moving Avg)\")\n",
    "lengths_avg = get_moving_avgs(test_episode_lengths, rolling_length)\n",
    "axs[1].plot(range(len(lengths_avg)), lengths_avg)\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Steps\")\n",
    "\n",
    "# Average reward per step\n",
    "axs[2].set_title(\"Avg Reward per Step (Moving Avg)\")\n",
    "rps_avg = get_moving_avgs(avg_reward_per_step, rolling_length)\n",
    "axs[2].plot(range(len(rps_avg)), rps_avg)\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Reward / Step\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
