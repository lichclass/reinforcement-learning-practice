{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0f7ae0-2045-4be2-bc4f-5aee284c0182",
   "metadata": {},
   "source": [
    "# Please Win Blackjack\n",
    "\n",
    "**Episodes: 1,000,000**\n",
    "Results:\n",
    "- Win Rate: \n",
    "- Draw Rate: \n",
    "- Loss Rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431e6600-82f9-4d37-8aec-f9ac08ee6ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackjack-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, natural\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Train the agent using off-policy MC prediction\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m V_estimates \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_off_policy_mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Evaluation Function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 96\u001b[0m, in \u001b[0;36mtrain_off_policy_mc\u001b[0;34m(env, behavior_policy, target_policy, num_episodes)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m     95\u001b[0m     episode \u001b[38;5;241m=\u001b[39m generate_episode(env, behavior_policy)  \u001b[38;5;66;03m# Use explicitly passed behavior policy\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_value_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_policy\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass both policies\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Print progress occasionally\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mupdate_value_function\u001b[0;34m(episode, V, returns, target_policy, behavior_policy)\u001b[0m\n\u001b[1;32m     68\u001b[0m V[state] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(returns[state])  \u001b[38;5;66;03m# Update V(s)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Importance sampling ratio\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m W \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtarget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m behavior_policy(state)  \u001b[38;5;66;03m# pi(A|S) / b(A|S)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Stop updates if importance sampling weight is 0\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 120\u001b[0m, in \u001b[0;36mtarget_policy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtarget_policy\u001b[39m(state):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Target policy that we want to evaluate.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    - Action: 0 (Stick) if state >= 20, else 1 (Hit).\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to Initialize V(s) and Returns storage\n",
    "def initialize_v():\n",
    "    V = defaultdict(float)  # Value function initialized arbitrarily\n",
    "    returns = defaultdict(list)  # Returns storage for every state\n",
    "    return V, returns\n",
    "\n",
    "V, returns = initialize_v()\n",
    "\n",
    "# Generate an episode using the behavior policy\n",
    "def generate_episode(env, behavior_policy):\n",
    "    \"\"\"\n",
    "    Generates an episode following the behavior policy.\n",
    "    \n",
    "    Args:\n",
    "    - env: The RL environment.\n",
    "    - behavior_policy: A function that selects an action given a state.\n",
    "\n",
    "    Returns:\n",
    "    - episode: A list of (state, action, reward) tuples.\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = behavior_policy(state)  # Select action using behavior policy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "    return episode\n",
    "\n",
    "# Off-Policy Monte Carlo Update\n",
    "def update_value_function(episode, V, returns, target_policy, behavior_policy):\n",
    "    \"\"\"\n",
    "    Updates the value function using off-policy every-visit Monte Carlo with importance sampling.\n",
    "\n",
    "    Args:\n",
    "    - episode: A list of (state, action, reward) tuples.\n",
    "    - V: The value function to update.\n",
    "    - returns: A dictionary storing returns for each state.\n",
    "    - target_policy: The policy to evaluate.\n",
    "    - behavior_policy: The policy that generated the data.\n",
    "\n",
    "    Returns:\n",
    "    - Updated V.\n",
    "    \"\"\"\n",
    "    G = 0  # Initialize return\n",
    "    W = 1  # Initialize importance sampling weight\n",
    "    visited_states = set()\n",
    "\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = G + reward  # Compute return\n",
    "\n",
    "        if state not in visited_states:  # Every-visit MC\n",
    "            visited_states.add(state)\n",
    "            returns[state].append(G)\n",
    "            V[state] = np.mean(returns[state])  # Update V(s)\n",
    "\n",
    "            # Importance sampling ratio\n",
    "            W *= target_policy(state) / behavior_policy(state)  # pi(A|S) / b(A|S)\n",
    "\n",
    "            if W == 0:\n",
    "                break  # Stop updates if importance sampling weight is 0\n",
    "\n",
    "    return V\n",
    "\n",
    "# Training Function\n",
    "def train_off_policy_mc(env, behavior_policy, target_policy, num_episodes=100000):\n",
    "    \"\"\"\n",
    "    Trains an off-policy Monte Carlo agent.\n",
    "\n",
    "    Args:\n",
    "    - env: The RL environment.\n",
    "    - behavior_policy: The policy used to generate episodes.\n",
    "    - target_policy: The policy being evaluated.\n",
    "    - num_episodes: Number of episodes for training.\n",
    "\n",
    "    Returns:\n",
    "    - V: Estimated value function.\n",
    "    \"\"\"\n",
    "    V, returns = initialize_v()\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(env, behavior_policy)  # Use explicitly passed behavior policy\n",
    "        V = update_value_function(episode, V, returns, target_policy, behavior_policy)  # Pass both policies\n",
    "\n",
    "        # Print progress occasionally\n",
    "        if episode_num % 10000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "    return V\n",
    "\n",
    "# Define the behavior policy (b)\n",
    "def behavior_policy(state):\n",
    "    \"\"\"\n",
    "    Behavior policy that generates episodes.\n",
    "    Returns:\n",
    "    - Action: Random choice between Stick (0) and Hit (1).\n",
    "    \"\"\"\n",
    "    return random.choice([0, 1])  # Random action selection\n",
    "\n",
    "# Define the target policy (pi)\n",
    "def target_policy(state):\n",
    "    \"\"\"\n",
    "    Target policy that we want to evaluate.\n",
    "    Returns:\n",
    "    - Action: 0 (Stick) if state >= 20, else 1 (Hit).\n",
    "    \"\"\"\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    return 0 if player_sum >= 20 else 1  # Stick at 20 or higher\n",
    "\n",
    "# Create Blackjack environment\n",
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
    "\n",
    "# Train the agent using off-policy MC prediction\n",
    "V_estimates = train_off_policy_mc(env, behavior_policy, target_policy)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_value_function(V):\n",
    "    \"\"\"\n",
    "    Prints the estimated value function.\n",
    "\n",
    "    Args:\n",
    "    - V: The estimated value function.\n",
    "    \"\"\"\n",
    "    print(\"\\nEstimated Value Function:\")\n",
    "    for state in sorted(V.keys()):\n",
    "        print(f\"V({state}) = {V[state]:.3f}\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "evaluate_value_function(V_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840a7c1-f78c-40a1-a0c2-1c2b9862fe19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
