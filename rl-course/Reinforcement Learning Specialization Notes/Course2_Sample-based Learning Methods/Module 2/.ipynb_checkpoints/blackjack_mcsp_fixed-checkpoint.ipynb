{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52ad05b-83de-46c8-98cf-65df1eddf01b",
   "metadata": {},
   "source": [
    "# Please Win Blackjack\n",
    "\n",
    "**Episodes: 1,000,000**\n",
    "Results:\n",
    "- Win Rate: 41.78% :)\n",
    "- Draw Rate: 8.44%\n",
    "- Loss Rate: 49.78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcbad7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000000 completed. Current epsilon: 0.1000\n",
      "Episode 50000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 100000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 150000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 200000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 250000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 300000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 350000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 400000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 450000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 500000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 550000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 600000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 650000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 700000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 750000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 800000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 850000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 900000/1000000 completed. Current epsilon: 0.0100\n",
      "Episode 950000/1000000 completed. Current epsilon: 0.0100\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to Initialize the Q-table, policy, and returns\n",
    "def initialize_mc_soft():\n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    returns = defaultdict(list)\n",
    "    policy = defaultdict(lambda: {0: 0.5, 1: 0.5})  # Initialize equal probabilities\n",
    "\n",
    "    return Q, returns, policy\n",
    "\n",
    "Q, returns, policy = initialize_mc_soft()\n",
    "\n",
    "# Generate Episode with Epsilon-Soft Policy\n",
    "def generate_episode(env, Q, epsilon=0.1):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Choose action using epsilon-soft policy\n",
    "        if state in Q and np.any(Q[state]):  # If Q-values exist for this state\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice([0, 1])  # Random action (exploration)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])  # Greedy action (exploitation)\n",
    "        else:\n",
    "            action = random.choice([0, 1])  # Random action if state unseen\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "    return episode\n",
    "\n",
    "# Monte Carlo Update with First-Visit Control and Policy Fixes\n",
    "def update_q_policy(episode, Q, returns, policy, epsilon=0.1):\n",
    "    G = 0\n",
    "    visited = set()  # Track first-visit state-action pairs\n",
    "\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + G  # Compute return\n",
    "\n",
    "        if (state, action) not in visited:  # First-visit check\n",
    "            visited.add((state, action))  # Mark as visited\n",
    "            returns[(state, action)].append(G)  # Store return\n",
    "            Q[state][action] = np.mean(returns[(state, action)])  # Average return\n",
    "\n",
    "            # Find the best action (greedy action)\n",
    "            A_star = np.argmax(Q[state])\n",
    "\n",
    "            # Ensure policy dictionary structure exists\n",
    "            if state not in policy:\n",
    "                policy[state] = {0: 0.5, 1: 0.5}\n",
    "\n",
    "            # Update the epsilon-soft policy\n",
    "            for a in [0, 1]:  # Stick (0) or Hit (1)\n",
    "                if a == A_star:\n",
    "                    policy[state][a] = 1 - epsilon + (epsilon / 2)  # Best action gets highest probability\n",
    "                else:\n",
    "                    policy[state][a] = epsilon / 2  # Other action gets small probability\n",
    "\n",
    "    return Q, policy\n",
    "\n",
    "# Training Function with Epsilon Decay\n",
    "def train_mc_soft(env, num_episodes=1000000, epsilon_start=0.1, epsilon_decay=0.99, epsilon_min=0.01):\n",
    "    Q, returns, policy = initialize_mc_soft()\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** episode_num))  # Decay epsilon over time\n",
    "\n",
    "        episode = generate_episode(env, Q, epsilon)\n",
    "        Q, policy = update_q_policy(episode, Q, returns, policy, epsilon)\n",
    "\n",
    "        # Print progress occasionally\n",
    "        if episode_num % 50000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed. Current epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    return Q, policy\n",
    "\n",
    "# Create Blackjack environment\n",
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
    "\n",
    "# Train the agent with fixed Monte Carlo Control using epsilon-soft policy\n",
    "Q, policy = train_mc_soft(env)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869fe94d-c695-4ce0-b5a8-029c1e4c8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Evaluates the learned policy by playing multiple episodes.\n",
    "    \n",
    "    Args:\n",
    "    - env: The Blackjack environment.\n",
    "    - policy: The learned epsilon-soft policy.\n",
    "    - num_episodes: Number of episodes to test the policy.\n",
    "\n",
    "    Returns:\n",
    "    - win_rate: Percentage of games won.\n",
    "    - draw_rate: Percentage of games drawn.\n",
    "    - loss_rate: Percentage of games lost.\n",
    "    \"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # Get action from learned policy (if state is known, otherwise pick randomly)\n",
    "            if state in policy:\n",
    "                action = np.argmax(list(policy[state].values()))  # Choose action with highest probability\n",
    "            else:\n",
    "                action = random.choice([0, 1])  # Random action if state is unseen\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if reward == 1:\n",
    "                    wins += 1\n",
    "                elif reward == 0:\n",
    "                    draws += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                break\n",
    "            \n",
    "            state = next_state  # Move to next state\n",
    "\n",
    "    # Compute win, draw, and loss rates\n",
    "    win_rate = wins / num_episodes\n",
    "    draw_rate = draws / num_episodes\n",
    "    loss_rate = losses / num_episodes\n",
    "\n",
    "    return win_rate, draw_rate, loss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7430710-f130-4232-bed4-064f0223ea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate: 41.78%\n",
      "Draw Rate: 8.44%\n",
      "Loss Rate: 49.78%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "win_rate, draw_rate, loss_rate = evaluate_policy(env, policy, 10000)\n",
    "\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Draw Rate: {draw_rate:.2%}\")\n",
    "print(f\"Loss Rate: {loss_rate:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
