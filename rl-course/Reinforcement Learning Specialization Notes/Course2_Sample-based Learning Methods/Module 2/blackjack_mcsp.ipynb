{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40fb23f-2d43-4178-9b28-10ae907c31c7",
   "metadata": {},
   "source": [
    "# Please Win Blackjack\n",
    "\n",
    "**Episodes: 1,000,000**\n",
    "Results:\n",
    "- Win Rate: \n",
    "- Draw Rate: \n",
    "- Loss Rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c8e9d6-1ec1-4dd4-8e96-974cc8e65323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ca9e82-aa20-482c-83d2-036eb52ec574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Initialize the Q-table, policy, and returns\n",
    "def initialize_mc_soft():\n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    returns = defaultdict(list)\n",
    "    policy = {}\n",
    "\n",
    "    return Q, returns, policy\n",
    "\n",
    "Q, returns, policy = initialize_mc_soft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c6168c-2659-4cc3-8a9d-5253069f00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, Q, epsilon=0.1):\n",
    "    episode = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Choose action using epsilon-soft policy\n",
    "        if state in Q and np.any(Q[state]): # If Q-values exist for this state\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice([0, 1]) # Random action (exploration)\n",
    "            else:\n",
    "                action = np.argmax(Q[state]) # Greedy action (exploitation)\n",
    "        else:\n",
    "            action = random.choice([0, 1]) # Random action if state unseen\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state # Move to the next state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad6b859-b0e0-4b70-9344-c546c610a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Update\n",
    "def update_q_policy(episode, Q, returns, policy, epsilon=0.1):\n",
    "    G = 0\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + G\n",
    "        returns[(state, action)].append(G)\n",
    "        Q[state][action] = np.mean(returns[(state, action)])\n",
    "\n",
    "        A_star = np.argmax(Q[state])\n",
    "\n",
    "        for a in [0, 1]: # Actions: 0 = Stick, 1 = Hit\n",
    "            if a == A_star:\n",
    "                policy[state][a] = 1 - epsilon + (epsilon / 2)\n",
    "            else:\n",
    "                policy[state][a] = epsilon / 2\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c8587f-4eec-42f3-8185-0f9b0f943009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mc_soft(env, num_episodes=500000, epsilon=0.1):\n",
    "    Q, returns, policy = initialize_mc_soft()\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(env, Q, epsilon)\n",
    "        Q, policy = update_q_policy(episode, Q, returns, policy, epsilon)\n",
    "\n",
    "        if episode_num % 100000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec11d99-3adf-49eb-b12d-c22c851c182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000000 completed.\n",
      "Episode 100000/1000000 completed.\n",
      "Episode 200000/1000000 completed.\n",
      "Episode 300000/1000000 completed.\n",
      "Episode 400000/1000000 completed.\n",
      "Episode 500000/1000000 completed.\n",
      "Episode 600000/1000000 completed.\n",
      "Episode 700000/1000000 completed.\n",
      "Episode 800000/1000000 completed.\n",
      "Episode 900000/1000000 completed.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create Blackjack environment\n",
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
    "\n",
    "# Train the agent\n",
    "Q, policy = train_mc_soft(env, num_episodes=1000000, epsilon=0.2)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195be3e8-5fb7-46b2-9695-fc257965a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Evaluates the learned policy by playing multiple episodes.\n",
    "    \n",
    "    Args:\n",
    "    - env: The Blackjack environment.\n",
    "    - policy: The learned epsilon-soft policy.\n",
    "    - num_episodes: Number of episodes to test the policy.\n",
    "\n",
    "    Returns:\n",
    "    - win_rate: Percentage of games won.\n",
    "    - draw_rate: Percentage of games drawn.\n",
    "    - loss_rate: Percentage of games lost.\n",
    "    \"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # Get action from learned policy (if state is known, otherwise pick randomly)\n",
    "            if state in policy:\n",
    "                action = np.argmax(list(policy[state].values()))  # Choose action with highest probability\n",
    "            else:\n",
    "                action = random.choice([0, 1])  # Random action if state is unseen\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if reward == 1:\n",
    "                    wins += 1\n",
    "                elif reward == 0:\n",
    "                    draws += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                break\n",
    "            \n",
    "            state = next_state  # Move to next state\n",
    "\n",
    "    # Compute win, draw, and loss rates\n",
    "    win_rate = wins / num_episodes\n",
    "    draw_rate = draws / num_episodes\n",
    "    loss_rate = losses / num_episodes\n",
    "\n",
    "    return win_rate, draw_rate, loss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41c2803-7dca-4451-8375-71e6d9dc54a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate: 28.85%\n",
      "Draw Rate: 4.33%\n",
      "Loss Rate: 66.82%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "win_rate, draw_rate, loss_rate = evaluate_policy(env, policy, 10000)\n",
    "\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Draw Rate: {draw_rate:.2%}\")\n",
    "print(f\"Loss Rate: {loss_rate:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
