{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c20a30-4b4d-45b4-bd56-2ea83c39b6b9",
   "metadata": {},
   "source": [
    "# Please Win Blackjack\n",
    "\n",
    "**Episodes:** 1,000,000 \\\n",
    "Results:\n",
    "- Win Rate: 38.42% :(\n",
    "- Draw Rate: 4.88% :|\n",
    "- Loss Rate: 56.70% ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc90fa76-3dbe-456f-855e-3c72cb65f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2c2447-cb04-46cc-8fa8-c672332d0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Initialize the Q-table, policy, and returns\n",
    "def initialize_mc_es():\n",
    "    Q = defaultdict(lambda: np.zeros(2)) # Action-value function (2 actions: hit or stick)\n",
    "    policy = {} # Stores the best action for each state\n",
    "    returns = defaultdict(list)\n",
    "    return Q, policy, returns\n",
    "\n",
    "Q, policy, returns = initialize_mc_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb3e668-b592-4681-a7b2-e4af098b6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a single episode\n",
    "def generate_episode(env, Q):\n",
    "    episode = []\n",
    "\n",
    "    # Exploring Starts: Choose a random (state, action) pair to begin\n",
    "    state, _ = env.reset() # Resets the environment and get a random initial state\n",
    "    action = random.choice([0, 1]) # 0 = Stick, 1 = Hit (random first action)\n",
    "\n",
    "    # Continue untile episode ends\n",
    "    while True: \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        if state in Q and np.any(Q[state]): # If we have Q-values, use greedy action\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = random.choice([0, 1])  # Random action if state is unseen\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83842216-3bd1-4ab2-abd1-d1f1510d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo ES Update\n",
    "def update_q_policy(episode, Q, returns, policy):\n",
    "    G = 0 # Initialize the return\n",
    "\n",
    "    # Process episode in reverse (backward updates)\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + G # Accumulate Rewards\n",
    "\n",
    "        # Store the return for this (state, action) pair\n",
    "        returns[(state, action)].append(G)\n",
    "\n",
    "        # Update Q-value using the average of all observed returns\n",
    "        Q[state, action] = np.mean(returns[state, action])\n",
    "\n",
    "        # Improve policy: Choose action with highest Q-value\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd1b6fd1-a0bc-4d95-accc-4a26a6876083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mc_es(env, num_episodes=10000):\n",
    "    Q, policy, returns = initialize_mc_es()\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(env, Q)\n",
    "\n",
    "        Q, policy = update_q_policy(episode, Q, returns, policy)\n",
    "\n",
    "         # Print progress occasionally\n",
    "        if episode_num % 100000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6565415e-db09-400e-ba0a-aaf1700819a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000000 completed.\n",
      "Episode 100000/1000000 completed.\n",
      "Episode 200000/1000000 completed.\n",
      "Episode 300000/1000000 completed.\n",
      "Episode 400000/1000000 completed.\n",
      "Episode 500000/1000000 completed.\n",
      "Episode 600000/1000000 completed.\n",
      "Episode 700000/1000000 completed.\n",
      "Episode 800000/1000000 completed.\n",
      "Episode 900000/1000000 completed.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural=True, sab=True)\n",
    "\n",
    "num_episodes = 1000000\n",
    "\n",
    "# Train the agent\n",
    "Q, policy = train_mc_es(env, num_episodes)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85ee63a0-74d5-4835-b4d8-19f91c42ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10000):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = policy[state] if state in policy else random.choice([0, 1])\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if reward == 1:\n",
    "                    wins += 1\n",
    "                elif reward == 0:\n",
    "                    draws += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    win_rate = wins / num_episodes\n",
    "    draw_rate = draws / num_episodes\n",
    "    loss_rate = losses / num_episodes\n",
    "\n",
    "    return win_rate, draw_rate, loss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa3c28a7-5a81-4f11-a326-3ffef319d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate: 38.42%\n",
      "Draw Rate: 4.88%\n",
      "Loss Rate: 56.70%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "win_rate, draw_rate, loss_rate = evaluate_policy(env, policy, num_episodes)\n",
    "\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Draw Rate: {draw_rate:.2%}\")\n",
    "print(f\"Loss Rate: {loss_rate:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
